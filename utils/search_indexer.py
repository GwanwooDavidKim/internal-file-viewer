# -*- coding: utf-8 -*-
"""
ê²€ìƒ‰ ì¸ë±ì„œ ëª¨ë“ˆ (Search Indexer)

íŒŒì¼ ë‚´ìš©ì„ ì¸ë±ì‹±í•˜ê³  ë¹ ë¥¸ ì „ë¬¸ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""
import os
import re
import json
import time
import threading
from datetime import datetime
from typing import Dict, List, Any, Optional, Set, Tuple
from collections import defaultdict
import config
from utils.file_manager import FileManager


class SearchIndex:
    """
    ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    íŒŒì¼ì˜ ë‚´ìš©ì„ í† í°í™”í•˜ì—¬ ì—­ ì¸ë±ìŠ¤(inverted index)ë¥¼ êµ¬ì¶•í•˜ê³ ,
    ë¹ ë¥¸ ì „ë¬¸ ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """SearchIndex ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.index = defaultdict(set)  # ë‹¨ì–´ -> íŒŒì¼ ê²½ë¡œ ì§‘í•©
        self.file_info = {}  # íŒŒì¼ ê²½ë¡œ -> íŒŒì¼ ì •ë³´
        self.stop_words = self._load_stop_words()
        self.lock = threading.RLock()
    
    def _load_stop_words(self) -> Set[str]:
        """ë¶ˆìš©ì–´ ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        # í•œêµ­ì–´ì™€ ì˜ì–´ ê¸°ë³¸ ë¶ˆìš©ì–´
        korean_stop_words = {
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ',
            'ì€', 'ëŠ”', 'ì´ë‹¤', 'ìˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”',
            'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ'
        }
        
        english_stop_words = {
            'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
            'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
            'to', 'was', 'will', 'with', 'or', 'but', 'if', 'this', 'they'
        }
        
        return korean_stop_words | english_stop_words
    
    def _tokenize(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        
        Args:
            text (str): ë¶„í• í•  í…ìŠ¤íŠ¸
            
        Returns:
            List[str]: í† í° ëª©ë¡
        """
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ì†Œë¬¸ì ë³€í™˜
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text.lower())
        
        # ê³µë°±ìœ¼ë¡œ ë¶„í• 
        tokens = text.split()
        
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§ (2ê¸€ì ì´ìƒ)
        filtered_tokens = [
            token for token in tokens 
            if len(token) >= 2 and token not in self.stop_words
        ]
        
        return filtered_tokens
    
    def add_file(self, file_path: str, content: str, file_info: Dict[str, Any]):
        """
        íŒŒì¼ì„ ì¸ë±ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): íŒŒì¼ ê²½ë¡œ
            content (str): íŒŒì¼ ë‚´ìš©
            file_info (Dict[str, Any]): íŒŒì¼ ì •ë³´
        """
        with self.lock:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ì—ì„œ í•´ë‹¹ íŒŒì¼ ì œê±°
            self.remove_file(file_path)
            
            # íŒŒì¼ ì •ë³´ ì €ì¥
            self.file_info[file_path] = {
                **file_info,
                'indexed_time': datetime.now(),
                'content_preview': content[:200] if content else '',
            }
            
            # íŒŒì¼ëª…ë„ ì¸ë±ì‹±ì— í¬í•¨
            filename = os.path.basename(file_path)
            all_content = f"{filename} {content}"
            
            # í…ìŠ¤íŠ¸ í† í°í™”
            tokens = self._tokenize(all_content)
            
            # ì—­ ì¸ë±ìŠ¤ êµ¬ì¶•
            for token in set(tokens):  # ì¤‘ë³µ ì œê±°
                self.index[token].add(file_path)
    
    def remove_file(self, file_path: str):
        """
        íŒŒì¼ì„ ì¸ë±ìŠ¤ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): ì œê±°í•  íŒŒì¼ ê²½ë¡œ
        """
        with self.lock:
            # íŒŒì¼ ì •ë³´ ì œê±°
            if file_path in self.file_info:
                del self.file_info[file_path]
            
            # ì¸ë±ìŠ¤ì—ì„œ í•´ë‹¹ íŒŒì¼ ì œê±°
            to_remove = []
            for token, file_paths in self.index.items():
                if file_path in file_paths:
                    file_paths.discard(file_path)
                    if not file_paths:  # ë¹ˆ ì§‘í•©ì´ë©´ í† í° ì œê±°
                        to_remove.append(token)
            
            for token in to_remove:
                del self.index[token]
    
    def search(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        with self.lock:
            if not query.strip():
                return []
            
            # ì¿¼ë¦¬ í† í°í™”
            query_tokens = self._tokenize(query)
            if not query_tokens:
                return []
            
            # ê° í† í°ë³„ë¡œ ë§¤ì¹­ë˜ëŠ” íŒŒì¼ ì°¾ê¸°
            token_results = []
            for token in query_tokens:
                matching_files = set()
                
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í† í°
                if token in self.index:
                    matching_files.update(self.index[token])
                
                # ë¶€ë¶„ ì¼ì¹˜í•˜ëŠ” í† í° (ì ‘ë‘ì‚¬ ë§¤ì¹­)
                for indexed_token in self.index:
                    if indexed_token.startswith(token) or token in indexed_token:
                        matching_files.update(self.index[indexed_token])
                
                token_results.append(matching_files)
            
            if not token_results:
                return []
            
            # AND ì—°ì‚° (ëª¨ë“  í† í°ì´ í¬í•¨ëœ íŒŒì¼)
            result_files = token_results[0]
            for token_result in token_results[1:]:
                result_files &= token_result
            
            # ê²°ê³¼ê°€ ì ìœ¼ë©´ OR ì—°ì‚°ë„ í¬í•¨
            if len(result_files) < max_results // 2:
                or_results = set()
                for token_result in token_results:
                    or_results |= token_result
                
                # AND ê²°ê³¼ë¥¼ ìš°ì„ í•˜ê³  OR ê²°ê³¼ë¥¼ ì¶”ê°€
                result_files = list(result_files) + list(or_results - result_files)
            else:
                result_files = list(result_files)
            
            # ê²°ê³¼ ì œí•œ
            result_files = result_files[:max_results]
            
            # ê²€ìƒ‰ ê²°ê³¼ êµ¬ì„±
            search_results = []
            for file_path in result_files:
                if file_path in self.file_info:
                    file_info = self.file_info[file_path]
                    
                    # ë§¤ì¹­ëœ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    content_preview = file_info.get('content_preview', '')
                    highlighted_preview = self._highlight_matches(content_preview, query_tokens)
                    
                    result = {
                        'file_path': file_path,
                        'filename': os.path.basename(file_path),
                        'file_type': file_info.get('file_type', 'unknown'),
                        'file_size_mb': file_info.get('file_size_mb', 0),
                        'indexed_time': file_info.get('indexed_time'),
                        'preview': highlighted_preview,
                        'relevance_score': self._calculate_relevance(file_path, query_tokens)
                    }
                    search_results.append(result)
            
            # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
            search_results.sort(key=lambda x: x['relevance_score'], reverse=True)
            
            return search_results
    
    def _highlight_matches(self, text: str, query_tokens: List[str]) -> str:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë§¤ì¹­ëœ ë¶€ë¶„ì„ í•˜ì´ë¼ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            text (str): ì›ë³¸ í…ìŠ¤íŠ¸
            query_tokens (List[str]): ê²€ìƒ‰ í† í°ë“¤
            
        Returns:
            str: í•˜ì´ë¼ì´íŠ¸ëœ í…ìŠ¤íŠ¸
        """
        highlighted = text
        
        for token in query_tokens:
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ë§¤ì¹­
            pattern = re.compile(re.escape(token), re.IGNORECASE)
            highlighted = pattern.sub(f"**{token}**", highlighted)
        
        return highlighted
    
    def _calculate_relevance(self, file_path: str, query_tokens: List[str]) -> float:
        """
        íŒŒì¼ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): íŒŒì¼ ê²½ë¡œ
            query_tokens (List[str]): ê²€ìƒ‰ í† í°ë“¤
            
        Returns:
            float: ê´€ë ¨ì„± ì ìˆ˜
        """
        if file_path not in self.file_info:
            return 0.0
        
        score = 0.0
        filename = os.path.basename(file_path).lower()
        
        for token in query_tokens:
            # íŒŒì¼ëª… ë§¤ì¹­ ë³´ë„ˆìŠ¤
            if token in filename:
                score += 2.0
            
            # í† í° ë¹ˆë„ ì ìˆ˜
            if token in self.index:
                # ë“œë¬¼ê²Œ ë‚˜íƒ€ë‚˜ëŠ” í† í°ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
                frequency = len(self.index[token])
                if frequency > 0:
                    score += 1.0 / frequency
        
        return score
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Dict[str, Any]: í†µê³„ ì •ë³´
        """
        with self.lock:
            return {
                'total_files': len(self.file_info),
                'total_tokens': len(self.index),
                'average_tokens_per_file': len(self.index) / max(len(self.file_info), 1),
                'file_types': self._get_file_type_distribution(),
            }
    
    def _get_file_type_distribution(self) -> Dict[str, int]:
        """íŒŒì¼ íƒ€ì…ë³„ ë¶„í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        distribution = defaultdict(int)
        for file_info in self.file_info.values():
            file_type = file_info.get('file_type', 'unknown')
            distribution[file_type] += 1
        return dict(distribution)


class SearchIndexer:
    """
    ê²€ìƒ‰ ì¸ë±ì„œ ë©”ì¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    íŒŒì¼ ì‹œìŠ¤í…œì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ìë™ìœ¼ë¡œ ì¸ë±ì‹±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """SearchIndexer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.file_manager = FileManager()
        self.index = SearchIndex()
        self.indexing_thread = None
        self.stop_indexing = False
        self.indexed_paths = set()
    
    def index_directory(self, directory_path: str, recursive: bool = True, 
                       progress_callback=None):
        """
        ë””ë ‰í† ë¦¬ë¥¼ ì¸ë±ì‹±í•©ë‹ˆë‹¤.
        
        Args:
            directory_path (str): ì¸ë±ì‹±í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
            recursive (bool): í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨ ì—¬ë¶€
            progress_callback: ì§„í–‰ ìƒíƒœ ì½œë°± í•¨ìˆ˜
        """
        if not os.path.exists(directory_path):
            return
        
        print(f"ğŸ“‚ ë””ë ‰í† ë¦¬ ì¸ë±ì‹± ì‹œì‘: {directory_path}")
        start_time = time.time()
        indexed_count = 0
        
        try:
            # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
            files_to_index = []
            
            if recursive:
                for root, dirs, files in os.walk(directory_path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        if self.file_manager.is_supported_file(file_path):
                            # ì—‘ì…€ íŒŒì¼ì€ ì¸ë±ì‹±ì—ì„œ ì œì™¸ (ì„±ëŠ¥ìƒ ì´ìœ )
                            file_type = self.file_manager.get_file_type(file_path)
                            if file_type != 'excel':
                                files_to_index.append(file_path)
            else:
                for item in os.listdir(directory_path):
                    file_path = os.path.join(directory_path, item)
                    if os.path.isfile(file_path) and self.file_manager.is_supported_file(file_path):
                        # ì—‘ì…€ íŒŒì¼ì€ ì¸ë±ì‹±ì—ì„œ ì œì™¸ (ì„±ëŠ¥ìƒ ì´ìœ )
                        file_type = self.file_manager.get_file_type(file_path)
                        if file_type != 'excel':
                            files_to_index.append(file_path)
            
            total_files = len(files_to_index)
            print(f"ğŸ“„ ì¸ë±ì‹± ëŒ€ìƒ íŒŒì¼: {total_files}ê°œ")
            
            # íŒŒì¼ë³„ ì¸ë±ì‹±
            for i, file_path in enumerate(files_to_index):
                if self.stop_indexing:
                    break
                
                try:
                    # íŒŒì¼ ì •ë³´ ì¡°íšŒ
                    file_info = self.file_manager.get_file_info(file_path)
                    
                    if file_info.get('supported', False):
                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        content = self.file_manager.extract_text(file_path)
                        
                        # ì¸ë±ìŠ¤ì— ì¶”ê°€
                        self.index.add_file(file_path, content, file_info)
                        self.indexed_paths.add(file_path)
                        indexed_count += 1
                        
                        # ì§„í–‰ ìƒíƒœ ì½œë°±
                        if progress_callback:
                            progress = (i + 1) / total_files * 100
                            progress_callback(file_path, progress)
                
                except Exception as e:
                    print(f"âŒ íŒŒì¼ ì¸ë±ì‹± ì˜¤ë¥˜ ({file_path}): {e}")
            
            elapsed_time = time.time() - start_time
            print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {indexed_count}ê°œ íŒŒì¼, {elapsed_time:.2f}ì´ˆ ì†Œìš”")
            
        except Exception as e:
            print(f"âŒ ë””ë ‰í† ë¦¬ ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
    
    def search_files(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:
        """
        íŒŒì¼ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        return self.index.search(query, max_results)
    
    def add_file_to_index(self, file_path: str):
        """
        ê°œë³„ íŒŒì¼ì„ ì¸ë±ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): ì¶”ê°€í•  íŒŒì¼ ê²½ë¡œ
        """
        try:
            if self.file_manager.is_supported_file(file_path):
                # ì—‘ì…€ íŒŒì¼ì€ ì¸ë±ì‹±ì—ì„œ ì œì™¸ (ì„±ëŠ¥ìƒ ì´ìœ )
                file_type = self.file_manager.get_file_type(file_path)
                if file_type == 'excel':
                    print(f"âš ï¸ ì—‘ì…€ íŒŒì¼ì€ ì¸ë±ì‹±ì—ì„œ ì œì™¸ë¨: {file_path}")
                    return
                
                file_info = self.file_manager.get_file_info(file_path)
                
                if file_info.get('supported', False):
                    content = self.file_manager.extract_text(file_path)
                    self.index.add_file(file_path, content, file_info)
                    self.indexed_paths.add(file_path)
                    print(f"âœ… íŒŒì¼ ì¸ë±ì‹± ì™„ë£Œ: {file_path}")
        
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì¸ë±ì‹± ì˜¤ë¥˜ ({file_path}): {e}")
    
    def remove_file_from_index(self, file_path: str):
        """
        íŒŒì¼ì„ ì¸ë±ìŠ¤ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): ì œê±°í•  íŒŒì¼ ê²½ë¡œ
        """
        self.index.remove_file(file_path)
        self.indexed_paths.discard(file_path)
        print(f"ğŸ—‘ï¸ íŒŒì¼ ì¸ë±ìŠ¤ ì œê±°: {file_path}")
    
    def update_file_in_index(self, file_path: str):
        """
        íŒŒì¼ ì¸ë±ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): ì—…ë°ì´íŠ¸í•  íŒŒì¼ ê²½ë¡œ
        """
        if file_path in self.indexed_paths:
            self.remove_file_from_index(file_path)
        
        self.add_file_to_index(file_path)
    
    def get_index_statistics(self) -> Dict[str, Any]:
        """
        ì¸ë±ìŠ¤ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Dict[str, Any]: í†µê³„ ì •ë³´
        """
        stats = self.index.get_statistics()
        stats['indexed_paths_count'] = len(self.indexed_paths)
        return stats
    
    def clear_index(self):
        """ì¸ë±ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.index = SearchIndex()
        self.indexed_paths.clear()
        print("ğŸ§¹ ê²€ìƒ‰ ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def stop_indexing_process(self):
        """ì¸ë±ì‹± í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤."""
        self.stop_indexing = True